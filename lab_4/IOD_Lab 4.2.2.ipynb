{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9XIMscuZXR3_"
   },
   "source": [
    "<div>\n",
    "<img src=https://www.institutedata.com/wp-content/uploads/2019/10/iod_h_tp_primary_c.svg width=\"300\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_MEq0z1KXR4C"
   },
   "source": [
    "## Lab 4.2.2: Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Izj4LsbrGNc8"
   },
   "source": [
    "Moving beyond basic feature selection methods, this lab introduces forward feature selection. Through an iterative process, we progressively include features that contribute to improving the model's adjusted R-squared score. By systematically evaluating the impact of each feature, we aim to construct a regression model that captures the underlying patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "Od_2rcZkXR4D"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mADumyI3XR4G"
   },
   "source": [
    "### 5. Forward Feature Selection\n",
    "\n",
    "> Forward Selection: Forward selection is an iterative method in which we start with having no feature in the model. In each iteration, we keep adding the feature which best improves our model till an addition of a new variable does not improve the performance of the model.\n",
    "\n",
    "Create a Regression model using Forward Feature Selection by looping over all the features adding one at a time until there are no improvements on the prediction metric ( R2  and  AdjustedR2  in this case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ha-2bFwdXR4H"
   },
   "source": [
    "#### 5.1 Load Wine Data & Define Predictor and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "dKp1loQLXR4H"
   },
   "outputs": [],
   "source": [
    "## Load the wine quality dataset\n",
    "\n",
    "# Load the wine dataset from csv\n",
    "wine = pd.read_csv('winequality_merged.csv')\n",
    "\n",
    "# define the target variable (dependent variable) as y\n",
    "y = wine['quality']\n",
    "\n",
    "# Take all columns except target as predictor columns\n",
    "predictor_columns = [c for c in wine.columns if c != 'quality']\n",
    "# Load the dataset as a pandas data frame\n",
    "X = pd.DataFrame(wine, columns = predictor_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "_IaxFgFkXR4K"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (5197, 12)\n",
      "X_test shape: (1300, 12)\n",
      "y_train shape: (5197,)\n",
      "y_test shape: (1300,)\n"
     ]
    }
   ],
   "source": [
    "## Create training and testing subsets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "print(f'X_train shape: {X_train.shape}')\n",
    "print(f'X_test shape: {X_test.shape}')\n",
    "print(f'y_train shape: {y_train.shape}')\n",
    "print(f'y_test shape: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sHGOCW66XR4M"
   },
   "source": [
    "#### 5.2 Overview of the code below\n",
    "\n",
    "The external `while` loop goes forever until there are no improvements to the model, which is controlled by the flag `changed` (until is **not** changed).\n",
    "The inner `for` loop goes over each of the features not yet included in the model and calculates the correlation coefficient. If any model improves on the previous best model then the records are updated.\n",
    "\n",
    "#### Code variables\n",
    "- `included`: list of the features (predictors) that were included in the model; starts empty.\n",
    "- `excluded`: list of features that have **not** been included in the model; starts as the full list of features.\n",
    "- `best`: dictionary to keep record of the best model found at any stage; starts 'empty'.\n",
    "- `model`: object of class LinearRegression, with default values for all parameters.\n",
    "\n",
    "#### Methods of the `LinearRegression` object to investigate\n",
    "- `fit()`\n",
    "- `fit.score()`\n",
    "\n",
    "#### Adjusted $R^2$ formula\n",
    "$$Adjusted \\; R^2 = 1 - { (1 - R^2) (n - 1)  \\over n - k - 1 }$$\n",
    "\n",
    "#### Linear Regression [reference](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "LVJY9yXaXR4M"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating feature: fixed acidity, Adjusted R2: 0.0059\n",
      "Evaluating feature: volatile acidity, Adjusted R2: 0.0753\n",
      "Evaluating feature: citric acid, Adjusted R2: 0.0077\n",
      "Evaluating feature: residual sugar, Adjusted R2: 0.0011\n",
      "Evaluating feature: chlorides, Adjusted R2: 0.0420\n",
      "Evaluating feature: free sulfur dioxide, Adjusted R2: 0.0027\n",
      "Evaluating feature: total sulfur dioxide, Adjusted R2: 0.0012\n",
      "Evaluating feature: density, Adjusted R2: 0.0987\n",
      "Evaluating feature: pH, Adjusted R2: 0.0001\n",
      "Evaluating feature: sulphates, Adjusted R2: 0.0007\n",
      "Evaluating feature: alcohol, Adjusted R2: 0.1944\n",
      "Evaluating feature: red_wine, Adjusted R2: 0.0167\n",
      "Selected feature: alcohol, New Adjusted R2: 0.1944\n",
      "Evaluating feature: fixed acidity, Adjusted R2: 0.1956\n",
      "Evaluating feature: volatile acidity, Adjusted R2: 0.2604\n",
      "Evaluating feature: citric acid, Adjusted R2: 0.2038\n",
      "Evaluating feature: residual sugar, Adjusted R2: 0.2128\n",
      "Evaluating feature: chlorides, Adjusted R2: 0.2035\n",
      "Evaluating feature: free sulfur dioxide, Adjusted R2: 0.2127\n",
      "Evaluating feature: total sulfur dioxide, Adjusted R2: 0.2011\n",
      "Evaluating feature: density, Adjusted R2: 0.1943\n",
      "Evaluating feature: pH, Adjusted R2: 0.1957\n",
      "Evaluating feature: sulphates, Adjusted R2: 0.1955\n",
      "Evaluating feature: red_wine, Adjusted R2: 0.2069\n",
      "Selected feature: volatile acidity, New Adjusted R2: 0.2604\n",
      "Evaluating feature: fixed acidity, Adjusted R2: 0.2608\n",
      "Evaluating feature: citric acid, Adjusted R2: 0.2603\n",
      "Evaluating feature: residual sugar, Adjusted R2: 0.2659\n",
      "Evaluating feature: chlorides, Adjusted R2: 0.2603\n",
      "Evaluating feature: free sulfur dioxide, Adjusted R2: 0.2622\n",
      "Evaluating feature: total sulfur dioxide, Adjusted R2: 0.2617\n",
      "Evaluating feature: density, Adjusted R2: 0.2678\n",
      "Evaluating feature: pH, Adjusted R2: 0.2614\n",
      "Evaluating feature: sulphates, Adjusted R2: 0.2695\n",
      "Evaluating feature: red_wine, Adjusted R2: 0.2659\n",
      "Selected feature: sulphates, New Adjusted R2: 0.2695\n",
      "Evaluating feature: fixed acidity, Adjusted R2: 0.2693\n",
      "Evaluating feature: citric acid, Adjusted R2: 0.2696\n",
      "Evaluating feature: residual sugar, Adjusted R2: 0.2777\n",
      "Evaluating feature: chlorides, Adjusted R2: 0.2705\n",
      "Evaluating feature: free sulfur dioxide, Adjusted R2: 0.2724\n",
      "Evaluating feature: total sulfur dioxide, Adjusted R2: 0.2696\n",
      "Evaluating feature: density, Adjusted R2: 0.2728\n",
      "Evaluating feature: pH, Adjusted R2: 0.2697\n",
      "Evaluating feature: red_wine, Adjusted R2: 0.2705\n",
      "Selected feature: residual sugar, New Adjusted R2: 0.2777\n",
      "Evaluating feature: fixed acidity, Adjusted R2: 0.2776\n",
      "Evaluating feature: citric acid, Adjusted R2: 0.2781\n",
      "Evaluating feature: chlorides, Adjusted R2: 0.2781\n",
      "Evaluating feature: free sulfur dioxide, Adjusted R2: 0.2784\n",
      "Evaluating feature: total sulfur dioxide, Adjusted R2: 0.2805\n",
      "Evaluating feature: density, Adjusted R2: 0.2776\n",
      "Evaluating feature: pH, Adjusted R2: 0.2789\n",
      "Evaluating feature: red_wine, Adjusted R2: 0.2817\n",
      "Selected feature: red_wine, New Adjusted R2: 0.2817\n",
      "Evaluating feature: fixed acidity, Adjusted R2: 0.2821\n",
      "Evaluating feature: citric acid, Adjusted R2: 0.2822\n",
      "Evaluating feature: chlorides, Adjusted R2: 0.2830\n",
      "Evaluating feature: free sulfur dioxide, Adjusted R2: 0.2838\n",
      "Evaluating feature: total sulfur dioxide, Adjusted R2: 0.2819\n",
      "Evaluating feature: density, Adjusted R2: 0.2854\n",
      "Evaluating feature: pH, Adjusted R2: 0.2823\n",
      "Selected feature: density, New Adjusted R2: 0.2854\n",
      "Evaluating feature: fixed acidity, Adjusted R2: 0.2858\n",
      "Evaluating feature: citric acid, Adjusted R2: 0.2853\n",
      "Evaluating feature: chlorides, Adjusted R2: 0.2867\n",
      "Evaluating feature: free sulfur dioxide, Adjusted R2: 0.2873\n",
      "Evaluating feature: total sulfur dioxide, Adjusted R2: 0.2854\n",
      "Evaluating feature: pH, Adjusted R2: 0.2864\n",
      "Selected feature: free sulfur dioxide, New Adjusted R2: 0.2873\n",
      "Evaluating feature: fixed acidity, Adjusted R2: 0.2879\n",
      "Evaluating feature: citric acid, Adjusted R2: 0.2872\n",
      "Evaluating feature: chlorides, Adjusted R2: 0.2886\n",
      "Evaluating feature: total sulfur dioxide, Adjusted R2: 0.2896\n",
      "Evaluating feature: pH, Adjusted R2: 0.2880\n",
      "Selected feature: total sulfur dioxide, New Adjusted R2: 0.2896\n",
      "Evaluating feature: fixed acidity, Adjusted R2: 0.2899\n",
      "Evaluating feature: citric acid, Adjusted R2: 0.2894\n",
      "Evaluating feature: chlorides, Adjusted R2: 0.2909\n",
      "Evaluating feature: pH, Adjusted R2: 0.2903\n",
      "Selected feature: chlorides, New Adjusted R2: 0.2909\n",
      "Evaluating feature: fixed acidity, Adjusted R2: 0.2913\n",
      "Evaluating feature: citric acid, Adjusted R2: 0.2908\n",
      "Evaluating feature: pH, Adjusted R2: 0.2914\n",
      "Selected feature: pH, New Adjusted R2: 0.2914\n",
      "Evaluating feature: fixed acidity, Adjusted R2: 0.2952\n",
      "Evaluating feature: citric acid, Adjusted R2: 0.2914\n",
      "Selected feature: fixed acidity, New Adjusted R2: 0.2952\n",
      "Evaluating feature: citric acid, Adjusted R2: 0.2950\n",
      "Selected features: ['alcohol', 'volatile acidity', 'sulphates', 'residual sugar', 'red_wine', 'density', 'free sulfur dioxide', 'total sulfur dioxide', 'chlorides', 'pH', 'fixed acidity']\n",
      "Final R2 on test set: 0.2936421875234342\n",
      "Final adjusted R2 on test set: 0.28760962856594796\n"
     ]
    }
   ],
   "source": [
    "## Flag intermediate output\n",
    "def adjusted_r2(r2, n, k):\n",
    "    return 1 - (1 - r2) * ((n - 1) / (n - k - 1))\n",
    "\n",
    "show_steps = True   # for testing/debugging\n",
    "show_steps = True  # Change to False for no intermediate output\n",
    "\n",
    "remaining_features = list(X_train.columns)\n",
    "selected_features = []\n",
    "current_score, best_new_score = 0, 0\n",
    "\n",
    "while remaining_features and current_score == best_new_score:\n",
    "    scores_with_candidates = []\n",
    "    for candidate in remaining_features:\n",
    "        model = LinearRegression().fit(X_train[selected_features + [candidate]], y_train)\n",
    "        r2 = r2_score(y_train, model.predict(X_train[selected_features + [candidate]]))\n",
    "        adj_r2 = adjusted_r2(r2, X_train.shape[0], len(selected_features) + 1)\n",
    "        scores_with_candidates.append((adj_r2, candidate))\n",
    "        \n",
    "        if show_steps:\n",
    "            print(f\"Evaluating feature: {candidate}, Adjusted R2: {adj_r2:.4f}\")\n",
    "\n",
    "    scores_with_candidates.sort()\n",
    "    best_new_score, best_candidate = scores_with_candidates.pop()\n",
    "    \n",
    "    if current_score < best_new_score:\n",
    "        remaining_features.remove(best_candidate)\n",
    "        selected_features.append(best_candidate)\n",
    "        current_score = best_new_score\n",
    "        \n",
    "        if show_steps:\n",
    "            print(f\"Selected feature: {best_candidate}, New Adjusted R2: {current_score:.4f}\")\n",
    "\n",
    "print(f\"Selected features: {selected_features}\")\n",
    "\n",
    "final_model = LinearRegression().fit(X_train[selected_features], y_train)\n",
    "final_r2 = r2_score(y_test, final_model.predict(X_test[selected_features]))\n",
    "final_adj_r2 = adjusted_r2(final_r2, X_test.shape[0], len(selected_features))\n",
    "\n",
    "print(f\"Final R2 on test set: {final_r2}\")\n",
    "print(f\"Final adjusted R2 on test set: {final_adj_r2}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# show_steps = False  # without showing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "vxROsvaIXR4P"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gillb\\anaconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_sequential.py:211: FutureWarning: Leaving `n_features_to_select` to None is deprecated in 1.0 and will become 'auto' in 1.3. To keep the same behaviour as with None (i.e. select half of the features) and avoid this warning, you should manually set `n_features_to_select='auto'` and set tol=None when creating an instance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE) on test set: 0.5409956167696763\n",
      "Mean Squared Error (MSE) with 5-fold cross-validation: 0.5476801133891139\n",
      "Selected Features:\n",
      "Index(['volatile acidity', 'residual sugar', 'density', 'sulphates', 'alcohol',\n",
      "       'red_wine'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "## Use Forward Feature Selection to pick a good model\n",
    "\n",
    "# Define the target variable (dependent variable) as y\n",
    "y = wine['quality']\n",
    "\n",
    "# Take all columns except 'quality' as predictor columns\n",
    "predictor_columns = [c for c in wine.columns if c != 'quality']\n",
    "X = wine[predictor_columns]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Forward feature selection using k-fold cross-validation\n",
    "# SequentialFeatureSelector with forward selection and k-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)  \n",
    "\n",
    "selector = SequentialFeatureSelector(model, n_features_to_select=None, direction='forward', cv=kf)\n",
    "\n",
    "# Fit the selector to the training data\n",
    "selector = selector.fit(X_train, y_train)\n",
    "\n",
    "# Get the selected feature indices\n",
    "selected_features = selector.get_support(indices=True)\n",
    "\n",
    "# Subset the training and testing data with selected features\n",
    "X_train_selected = X_train.iloc[:, selected_features]\n",
    "X_test_selected = X_test.iloc[:, selected_features]\n",
    "\n",
    "# Train the model on the selected features using cross-validation\n",
    "scores = cross_val_score(model, X_train_selected, y_train, cv=kf, scoring='neg_mean_squared_error')\n",
    "mse_cv = -scores.mean()\n",
    "\n",
    "# Train the model on the selected features (without cross-validation for final model)\n",
    "model.fit(X_train_selected, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test_selected)\n",
    "\n",
    "# Calculate the mean squared error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error (MSE) on test set: {mse}')\n",
    "print(f'Mean Squared Error (MSE) with {kf.n_splits}-fold cross-validation: {mse_cv}')\n",
    "\n",
    "# Print selected feature names\n",
    "selected_feature_names = X.columns[selected_features]\n",
    "print('Selected Features:')\n",
    "print(selected_feature_names)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3LCpYCPXl1XK"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "> > > > > > > > > Â© 2024 Institute of Data\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
